---
---
# (APPENDIX) Appendix {-} 

# Data Wrangling in R


:::tools

**Tools**

For this module, you need:  

- Your **toolkit journal**
- You may also want to generate a series of digital documents for final **Project Requirements** developed in this module. That may be a series of documents, spreadsheets, or templates identified across online resources.

:::

:::objective
**Objectives**

In this module, you will: 

- Introduction to spatial data concepts and operations
- Convert CSV lat/long to spatial points and geocode address
- Overlay points with boundary data, merge SDOH data, visualize as choropleth
:::

## Intro to Spatial Data

Spatial data is essential for understanding the world around us, as it combines information with specific locations. This type of data is vital because it allows us to see how information changes with location. Without the geographical component, we're left with just a list, not a map that can guide decisions or provide insights.

An important concept within spatial data is "simple features," which is an international standard for how we represent real-world objects and their shapes on computers. This standard is the backbone of many Geographic Information Systems (GIS) technologies, making it easier for us to map and analyze spatial data. For example, in the R programming environment, spatial objects can be stored in a way that's easy to work with, integrating seamlessly with other data analysis processes.

Understanding these aspects of spatial data is crucial, especially as we delve into more complex analyses. Whether it's navigating through the components of a shapefile or exploring data in R, having a grasp on these concepts can help tackle the challenges that come with spatial data analysis.

## Projections

The Earth is an irregular ellipsoid, rather than a perfect sphere. As a result, many ways of visualizing the Earth, in whole or in part, have been developed. A Coordinate Reference System (CRS) communicates what method should be used to flatten or project the Earth’s surface onto a 2-dimensional map. Different CRS imply different ways of projections and can generate substantially different visualizations. For example, the following are some world maps using different projections:

pic from slide 13 intro2spatialmed

In addition to projections that optimize for global accuracy, there are many regional projections. For instance, NAD83 (North American Datum 1983) is divided into several UTM (universal transverse mercator) zones across the continent. So when projecting a dataset of Illinois, for example, the most appropriate NAD83 projection would be NAD83 UTM zone 16N. Projection names have also been standardized to EPSG codes. The projection NAD83 UTM zone 16N is EPSG:26916.

Many datasets available online, and GeoJSON files as a general rule, default to the projection EPSG:4326, a global projection used by GPS. Changing projections can be critical for improving accuracy for your project, and making sure your layers are being displayed using the same information. Always search the internet to determine what projection is most appropriate for your project, and to find the corresponding EPSG code.

For this exercise we’ll use QGIS to change the projection of a spatial dataset. 

## Environmental Setup

Before starting operations related to spatial data, we need to complete an environmental setup. A basic understanding of R is assumed. This workshop requires several packages, which can be installed from CRAN:

```{r eval=FALSE}
install.packages("sf", "tmap", "tidygeocoder", "dplyr")
```

For Mac users, check out https://github.com/r-spatial/sf for additional tips if you run into errors when installing the **sf** package. Using homebrew to install **gdal** usually fixes any remaining issues.

Now, loading the libraries:

```{r load-libraries, warning=FALSE}
library(sf)
library(dplyr)
```

## Convert CSV Lat/Long to Points

Converting CSV latitude and longitude data to points in R is a straightforward yet powerful method for spatial analysis. This process involves using the `sf` package to transform geographic coordinates into spatial points, allowing for easy mapping and analysis. It's an essential step for anyone working with geospatial data, enabling the visualization of locations and the application of geographic information systems (GIS) techniques. By assigning a Coordinate Reference System (CRS), these points become ready for spatial operations.

We are using the "Affordable_Rental_Housing_Developments.csv" in the dataset to show how to convert a csv Lat/Long data to points. First, we need to load the CSV data:

```{r load-csv-data}
housing = read.csv("o4rtestdata/Affordable_Rental_Housing_Developments.csv")
```

Then, we need to ensure that no column (intended to be used as a coordinate) is entirely empty or filled with NA values:
```{r }
cleaned_housing <- na.omit(housing)
```

Finally, we start to convert it to points:
```{r }
points_housing <- st_as_sf(cleaned_housing, coords = c("Longitude", "Latitude"), crs = 3435)
```

If you want, you can view the resulting sf object:
```{r }
print(points_housing)
```

## Prepare Spatial Data

In this section, we will go through the basic environment setup and spatial data preparation for the further operations and analysis.

### Load Spatial Data

We need to load the spatial data (shapefile). All the data used for this one assignment can be found here [here](https://github.com/Makosak/Intro2RSpatialMed/tree/main/data). Remember, this type of data is actually comprised of multiple files. All need to be present in order to read correctly.

```{r load-spatial-data}
Chi_tracts = st_read("o4rtestdata/geo_export_aae47441-adab-4aca-8cb0-2e0c0114096e.shp")
```

Always inspect data when loading in. First we look at a non-spatial view.

```{r non-spatial-view}
head(Chi_tracts)
```

Note the last column -- this is a spatially enabled column. The data is no longer a 'shapefile' but an `sf' object, comprised of polygons.

We can use a baseR function to view the spatial dimension. The `sf` framework enables previews of each attribute in our spatial file. 

```{r spatial-view}
plot(Chi_tracts)
```

Check out the data structure of this file.

```{r data-structure}
str(Chi_tracts)

```

### Change Projections/CRS

A Coordinate Reference System (CRS) is crucial for mapping the Earth's three-dimensional surface onto a flat, two-dimensional map. It defines how to project the Earth's surface, which is vital because different CRSs can make the same location appear differently on maps. This means choosing the right CRS is essential for accurate spatial analysis and mapping. In practice, especially when using software like R, it's important to check and adjust the CRS of your data to ensure consistency across your project. This helps in avoiding misinterpretations and errors in your spatial analysis.

First, check out the coordinate reference system.

```{r }
st_crs(Chi_tracts)
```

To transform our CRS, we use the `st_transform` function. Let's choose a projection that is focused on Illinois, and uses distance as feet or meters, to make it a bit more accessible for our work. EPSG:3435 is a good fit:

```{r }
Chi_tracts.3435 <- st_transform(Chi_tracts, "EPSG:3435")
# Chi_tracts.3435 <- st_transform(Chi_tracts, 3435)
st_crs(Chi_tracts.3435)

plot(st_geometry(Chi_tracts.3435), border = "gray", lwd = 2, main = "NAD83 / Illinois East (ftUS)", sub="topo mapping & survey use")
```

### Refine Basic Map

Now we'll switch to a more extensive cartographic mapping package, `tmap`. We approach mapping with one layer at a time. Always start with the object you want to map by calling it with the `tm_shape` function. Then, at least one descriptive/styling function follows. There are hundreds of variations and paramater specifications, so take your time in exploring `tmap` and the options.

Here we style the tracts with some semi-transparent borders.
```{r }
library(tmap)

tm_shape(Chi_tracts) + tm_borders(alpha=0.5) 
```

Next we fill the tracts with a light gray, and adjust the color and transparency of borders. We also add a scale bar, positioning it to the left and having a thickness of 0.8 units, and turn off the frame.

```{r }
tm_shape(Chi_tracts) + tm_fill(col = "gray90") + tm_borders(alpha=0.2, col = "gray10") +
  tm_scale_bar(position = ("left"), lwd = 0.8) +
  tm_layout(frame = F)
```

Check out https://rdrr.io/cran/tmap/man/tm_polygons.html for more ideas!

### Overlay Zip Code Boundaries

How do census tract areas correspond to zip codes? While tracts better represent neighborhoods, often times we are stuck with zip code level scale in healh research. Here we'll make a reference map to highlight tract distribution across each zip code.

First, we read in zip code boundaries. This data was downloaded directly from the *City of Chicago Data Portal* as a shapefile.

```{r }
Chi_Zips = st_read("o4rtestdata/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp")

```

Next, we layer the new shape in -- on top of the tracts. We use a thicker border, and try out a new color. Experiment!

```{r }

## FIRST LAYER: CENSUS TRACT BOUNADRIES
tm_shape(Chi_tracts.3435) + tm_fill(col = "gray90") +
  tm_borders(alpha=0.2, col = "gray10") + 

## SECOND LAYER: ZIP CODE BOUNDARIES WITH LABEL
tm_shape(Chi_Zips) + tm_borders(lwd = 2, col = "#0099CC") +
  tm_text("zip", size = 0.7) +
  
## MORE CARTOGRAPHIC STYLE
  tm_scale_bar(position = ("left"), lwd = 0.8) +
  tm_layout(frame = F)
```

## Geocode Addresses to Points

### Prepare Address Data

If you start with only addresses, you'll need to geocode. Our methadone maintenance provider dataset is only available as such. Addresses are comprised of characeters that reference a specific place. We will use the network topology service of a *Geocoder* to translate that address to a coordinate in some CRS. 

First we load the `tidygeocoder` to get our geocoding done. Note, this uses the interent to process, so is not suitable for HIPPA protected data like individual, living person addresses. For offline geocoders, check out `Pelias` or `ESRI`.

```{r warning=FALSE}
library(tidygeocoder)
```

Let's read in and inspect data for methadone maintenance providers. Note, these addresses were made available by SAMSHA, and are known as publicly available information. An additional analysis could call each service to check on access to medication during COVID in Septmber 2020, and the list would be updated further.

```{r}
methadoneClinics <- read.csv("o4rtestdata/chicago_methadone_nogeometry.csv")
head(methadoneClinics)
```

Let's geocode one address first, just to make sure our system is working. We'll use the "cascade" method which use the US Census and OpenStreetMap geocoders. These two services are the main options with `tidygeocoder`.

```{r}
sample <- geo("2260 N. Elston Ave. Chicago, IL", lat = latitude, long = longitude, method = 'cascade')
head(sample)
```

As we prepare for geocoding, check out the structure of the dataset. The data should be a character to be read properly. 

```{r}
str(methadoneClinics)
```

We need to clean the data a bit. We'll add a new column for a full address, as required by the geocoding service. When you use a geocoding service, be sure to read the documentation and understand how the data needs to be formatted for input.

```{r}
methadoneClinics$fullAdd <- paste(as.character(methadoneClinics$Address), 
                                  as.character(methadoneClinics$City),
                                  as.character(methadoneClinics$State), 
                                  as.character(methadoneClinics$Zip))
```

We're ready to go! Batch geocode with one function, and inspect:

```{r}
geoCodedClinics <-  geocode(methadoneClinics,
address = 'fullAdd', lat = latitude, long = longitude, method = 'cascade')

head(geoCodedClinics)
```

There were two that didn't geocode correctly. You can inspect further. This could involve a quick check for spelling issues; or, searching the address and pulling the lat/long using Google Maps and inputting manually. Or, if we are concerned it's a human or unknown error, we could omit. For this exercise we'll just omit the two clinics that didn't geocode correctly.

```{r}
geoCodedClinics2 <- na.omit(geoCodedClinics)
```

### Convert to Spatial Data

This is not spatial data yet! To convert a static file to spatial data, we use the powerful `st_as_sf` function from `sf`. Indicate the x,y parameters (=longitude, latitude) and the coordinate reference system used. Our geocoding service used the standard **EPSG:4326**, so we input that here.

```{r warning = FALSE}
library(sf)

methadoneSf <- st_as_sf(geoCodedClinics2, 
                        coords = c( "longitude", "latitude"),
                        crs = 4326)
```

### Basic Map of Points 

For a really simple map of points -- to ensure they were geocoded and converted to spatial data correctly, we use `tmap`. We'll use the interactive version to view.

```{r warning = FALSE, message=FALSE}
library(tmap)

tmap_mode("view")

tm_shape(methadoneSf) + tm_dots() 
```

If your points didn't plot correctly:

- Did you flip the longitude/latitude values?
- Did you input the correct CRS?

Those two issues are the most common errors.

### Overlay Points & Style

Let's add our zip code map from the previous module. First load the data, then overlay.

```{r}
Chi_Zipsf <- st_read("o4rtestdata/ChiZipMaster1.geojson")
```

With this overlay, we'll add a "hack" to include the methadone clinic points in a legend.

```{r}
tmap_mode("plot")

## 1st layer (gets plotted first)
tm_shape(Chi_Zipsf) + tm_fill("Case.Rate...Cumulative", 
              style="jenks", pal="BuPu", n=4, title = "COVID Rt") + 
  
  ## 2nd layer (overlay)
  tm_shape(methadoneSf) + tm_dots(size = 0.2, col = "gray20") +
  
  ## "Hack" a manual symbology for dots in the legend
  tm_add_legend("symbol", col = "gray20", size = .2, labels = "Methadone MOUD") +
  
  ## Cartographic Styling
  tm_layout(legend.outside = TRUE, legend.outside.position = "right")
```


## Merge SDOH Data to Boundary Dataset

From here, we can integrate more data. Let's try a different point dataset -- Affordable Rental Housing Developments, as made available by the *City of Chicago Data Portal*. This could be interesting for a number of different reasons -- maybe we hypothesize better outcomes are associated with better access to affordable housing options? Or, we hypothesize the opposite, that mean distance to more population dense housing locations is vulnerable to airborne disease?

For this example, we'll think about this dataset as access to secure and affordable housing. Persons with lower incomes residing in places with fewer developments may be more vulnerable to housing insecurity -> impacts health.

```{r, warning=FALSE}
AffHousing <- read.csv("o4rtestdata/Affordable_Rental_Housing_Developments.csv")

head(AffHousing)
```

There were a few data points with odd inputs and null values. Remember, we can't convert any null values to spatial coordinates. Again, in an ideal context, you would explore and understand what is happening, systematically. In our experiment, we'll omit nulls.

```{r, warning=FALSE}
AffHousing <- na.omit(AffHousing)
```

Look at the structure of the object. 

```{r, warning=FALSE}
str(AffHousing)
```

In this dataset, we can see coordinate information is already included -- twice! You're looking at 2 different types of coordinate systems. We'll use "Longitude" and "Latitude" to represent X,Y and an ESPG of 4326. We're guessing, and hopeful.

```{r, warning=FALSE}
AffHousingSf <- st_as_sf(AffHousing, 
                        coords = c("Longitude", "Latitude"),
                        crs = 4326)
```

We can now map the data for a quick view -- does this look like Chicago, hopefully?

```{r, warning=FALSE}
tm_shape(AffHousingSf) + tm_bubbles("Units", col = "purple", style = "sd")  
```

## Graduated Symbology & Choropleth Map 

Previously we mapped points as dots. We literally used the `tm_dots()` function to do so. Another option is changing the size of the point, according to some attribute of the data. In this dataset, we see an attribute field that gives us the total number of units per housing site. Let's use a graduated symbology, with the `tm_bubbles()` function, to map these points. That way points with more units will be bigger, and not all places are weighted the same visually.

```{r}
tm_shape(Chi_Zipsf) + tm_polygons(col = "gray80") +
  tm_shape(AffHousingSf) + tm_bubbles("Units", col = "purple") 
```

Let's map everything at once, and explore which zip codes are the most vulnerable to persons with OUD during the pandemic in September 2020, based on the information we have here?

```{r}
## Zip Codes with Labels
tm_shape(Chi_Zipsf) + tm_fill("Case.Rate...Cumulative", 
      style="jenks", pal="BuPu", n=4, title = "COVID Rt")  +

  ## Affordable Housing Units
  tm_shape(AffHousingSf) + tm_bubbles("Units") + 
  
  ## Methadone MOUD
  tm_shape(methadoneSf) + tm_dots(size = 0.2, col = "gray20") +
  
  ## Cartographic Styling
  tm_add_legend("symbol", col = "gray20", size = .2, labels = "Methadone MOUD") + tm_layout(legend.outside = TRUE, legend.outside.position = "right")
```

In RStudio, you could zoom into the plot you created to get a better view. Save as an image, or save as a webpage!

Save any data you need from this session.

```{r eval=FALSE}
st_write(methadoneSf, dsn = "o4rtestdata/methadoneMOUD.geojson")
```
